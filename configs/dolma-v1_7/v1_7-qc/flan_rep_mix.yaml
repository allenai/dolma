streams:
  - name: tulu_flan
    documents:
      - s3://ai2-llm/pretraining-data/sources/tulu_flan/v1-decontaminated-60M-shots_all-upweight_1-dialog_false-sep_newline/documents/train/*
    attributes:
      - tokenizer_repetitions_v2r2
      - whitespace_tokenizer_v1
    output:
      max_size_in_bytes: 500_000_000
      path: s3://ai2-llm/pretraining-data/sources/tulu_flan/v12decontaminated-60M-shots_all-upweight_1-dialog_false-sep_newline/documents/train/*
      min_text_length: 25   # matches wikipedia
      # discard_fields:
      #   - attributes
      #   - metadata
    filter:
      exclude:
        # Remove repetitions
        - >-
          (.tokenizer_repetitions_v2r2__tokenizer_repetitions_v2r2__doc_max_score_repetition != null) and
          (.tokenizer_repetitions_v2r2__tokenizer_repetitions_v2r2__doc_max_score_repetition[0][-1] > 10)

        # Too few tokens
        - .attributes.whitespace_tokenizer_v1__whitespace_tokenizer_v1__length[0][-1] < 25

        # Too many tokens
        - .attributes.whitespace_tokenizer_v1__whitespace_tokenizer_v1__length[0][-1] > 5000
      syntax: jq

work_dir:
  input: "/tmp/tulu_flan_mix/input"
  output: "/tmp/tulu_flan_mix/output"

processes: 188
